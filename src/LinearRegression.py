# -*- coding: utf-8 -*-
"""Эконометрика

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gy1qjVljEG54RpMIGq5UQiKORFup2xhI
"""

# from google.colab import drive
# drive.mount("/content/drive/")
# import os
# os.chdir("/content/drive/MyDrive")

import numpy as np
import pandas as pd
import pandas as pd
import sklearn
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf

"""2. **Предварительная подготовка данных**

* Разделите вашу выборку на тестовую и обучающую, 20% и 80% соответстенно. 
* Вычислите по исходным данным показатели, необходимые для вашей модели (логарифмы, разности или отношения), создайте дамми и категориальные переменные (если это необходимо)
* Приведите описание выполненных операций.
"""


def load_dataset():
    return pd.read_csv("train.csv")


data = load_dataset()

# Применение условий фильтрации (1 комната, ЗАО)
cond_room = data['num_room'] == 1
cond_area = (data['sub_area'] == 'Dorogomilovo') | (data['sub_area'] == 'Krylatskoe') | (
            data['sub_area'] == 'Kuncevo') | \
            (data['sub_area'] == 'Mozhajskoe') | (data['sub_area'] == 'Novo-Peredelkino') | \
            (data['sub_area'] == 'Ochakovo-Matveevskoe') | (data['sub_area'] == 'Prospekt Vernadskogo') | \
            (data['sub_area'] == 'Ramenki') | (data['sub_area'] == 'Solncevo') | \
            (data['sub_area'] == 'Troparevo-Nikulino') | (data['sub_area'] == 'Filevskij Park') | \
            (data['sub_area'] == 'Fili Davydkovo') | (data['sub_area'] == 'Vnukovo')

# Отбираем показатели
data = data[
    ['price_doc', 'full_sq', 'life_sq', 'floor', 'max_floor', 'build_year', 'kremlin_km', 'public_transport_station_km',
     'metro_km_walk', 'park_km', 'green_zone_part', 'green_zone_km']]
data = data[cond_area & cond_room]

num_rows = data.shape[0]
print("\nКоличество строк в датасете:", num_rows)

null_sum = data.isnull().sum()
filtered_rows = null_sum[null_sum != 0]
print(filtered_rows)

# Удаляем пустые строки в датасете
data = data.dropna(subset=['build_year'])
data = data.dropna(subset=['metro_km_walk'])
data = data.dropna(subset=['life_sq'])

# Добавляем дамми - первый/последний этаж = 1, 0 в ином случае;
# близость к метро = 1, если расстояние до метро < 1.5 км, 0 в ином случае
data['first_last_floor'] = 0
data.loc[(data['floor'] == data['max_floor']) | (data['floor'] == 1), 'first_last_floor'] = 1

data['metro_close'] = 0
data.loc[(data['metro_km_walk'] <= 1.5), 'metro_close'] = 1
print(len(data[data['metro_close'] == 0]))

num_rows = data.shape[0]
print("\nКоличество строк в датасете:", num_rows)

# Сырые данные, но без NaN
data_raw = data.copy()

from sklearn.model_selection import train_test_split

X_raw = data_raw.drop(columns=['price_doc'])
y_raw = data_raw['price_doc']

X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_raw, y_raw, test_size=0.2, random_state=42)

print("Обучающая выборка, количество строк и столбцов:", X_train_raw.shape, y_train_raw.shape)
print("Тестовая выборка, количество строк и столбцов:", X_test_raw.shape, y_test_raw.shape)

cond_life_sq = (data['life_sq'] > 10) & (data['life_sq'] < 200)
cond_full_sq = (data['full_sq'] > 10) & (data['full_sq'] < 200)
data = data[cond_life_sq & cond_full_sq]

# выброс строки Тестовой выборки
data = data.drop(data[data['price_doc'] == 44000000].index)

data['price_doc'] = np.log(data['price_doc'])
data.head()

# Делим выборку на трейн/тест
X = data.drop(columns=['price_doc'])
y = data['price_doc']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Обучающая выборка, количество строк и столбцов:", X_train.shape, y_train.shape)
print("Тестовая выборка, количество строк и столбцов:", X_test.shape, y_test.shape)

"""3. **Визуальный анализ данных и анализ описательных статистик**


* Дайте подробный предварительный анализ ваших показателей (графический анализ, анализ описательных статистик). 
* Какие показатели в большей, а какие в меньшей степени истощают выборку?
* Какие показатели симметрично и асимметрично распределены?
* Какие показатели отличаются большим (маленьким) разбросом? (На этот вопрос позволяет ответить изучение коэффициента вариации – отношения стандартной ошибки к среднему)
* Какие показатели сильно (слабо) коррелированы между собой?
* По каким показателям есть нетипичные наблюдения?

"""

statistics = data.describe()
print(statistics.to_string(index=True))

data.head()

print("Минимальная цена квартиры:", np.exp(min(data['price_doc'])))
print("Максимальная цена квартиры:", np.exp(max(data['price_doc'])))
print("Средняя цена квартиры:", np.exp(np.mean(data['price_doc'])))

import matplotlib.pyplot as plt

data.hist(bins=10, figsize=(12, 14))
plt.show()

mean = np.mean(data, axis=0)
std = np.std(data, axis=0)
cv = (std / mean) * 100

print()
for i, col_name in enumerate(data):
    print("Коэффициент вариации показателя", col_name, "=", cv[i])

corr_matrix = data.corr()
plt.figure(figsize=(13, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()
print()
"""4. **Поиск наилучшей функциональной формы**
* Какое распределение имеет зависимая переменная? 
* Стоит ли корректировать выбранные переменные для дальнейшего анализа?
* Проведите необходимые действия и тесты для выбора наиболее адекватной функциональной формы
* Значима ли наиболее адекватная регрессия в целом? 
* Какие регрессоры значимы? 
* Какой процент разброса зависимой переменной удалось объяснить? 
* Дайте обоснованные соответствующими количественными показателями ответы на эти вопросы. Проинтерпретируйте содержательно полученные результаты.
"""
# Очищаем данные от выбросов в build_year, price_doc

data = data.drop(data[data.build_year < 3].index)

# Боксплоты комментированы.

# data.boxplot(column=['build_year'])

# data.boxplot(column=['price_doc'])

data = data.drop(data[data.price_doc < 15].index)
data = data.drop(data[data.price_doc > 16.5].index)

# data.boxplot(column=['price_doc'])

# Очищенные от выбросов данные

X_train_clean = X_train.copy()
y_train_clean = y_train.copy()

X_test_clean = X_test.copy()
y_test_clean = y_test.copy()

y_test_clean.head()

# Шапиро_Уилка, лог. модель
from scipy.stats import shapiro

# perform Shapiro-Wilk test
shapiro(data['price_doc'])
print()
import statsmodels.api as sm

X = data[
    ['full_sq', 'floor', 'max_floor', 'first_last_floor', 'build_year', 'kremlin_km', 'public_transport_station_km',
     'metro_close', 'metro_km_walk', 'park_km', 'green_zone_part', 'green_zone_km']]
y = data['price_doc']

X = sm.add_constant(X)

model = sm.OLS(y, X)
results = model.fit()

residuals = results.resid
fitted_values = results.fittedvalues

plt.scatter(fitted_values, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Fitted values')
plt.ylabel('Residuals')
plt.title('Остатки логарифмической модели')
plt.show()
print()
from statsmodels.graphics.gofplots import qqplot

qq_data = residuals

qqplot(qq_data, line='s')  # 's' - для построения стандартной линии
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Sample Quantiles')
plt.title('График QQ логарифмической модели')
plt.show()

import statsmodels.api as sm

X = data[
    ['full_sq', 'floor', 'max_floor', 'first_last_floor', 'build_year', 'kremlin_km', 'public_transport_station_km',
     'metro_close', 'metro_km_walk', 'park_km', 'green_zone_part', 'green_zone_km']]
y = data['price_doc']

X = sm.add_constant(X)

model = sm.OLS(y, X)

results = model.fit()

print(results.summary())

data_lin = data.copy()
data_lin['price_doc'] = np.exp(data_lin['price_doc'])
data_lin.head()

# Шапиро_Уилка, лин. модель
# perform Shapiro-Wilk test
shapiro(data_lin['price_doc'])

import statsmodels.api as sm

X = data_lin[
    ['full_sq', 'floor', 'max_floor', 'first_last_floor', 'build_year', 'kremlin_km', 'public_transport_station_km',
     'metro_close', 'metro_km_walk', 'park_km', 'green_zone_part', 'green_zone_km']]
y = data_lin['price_doc']

X = sm.add_constant(X)
model_lin = sm.OLS(y, X)

results_lin = model_lin.fit()

residuals_lin = results_lin.resid
fitted_values_lin = results_lin.fittedvalues

print(results_lin.summary())
print()
plt.scatter(fitted_values_lin, residuals_lin)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Fitted values_lin')
plt.ylabel('Residuals_lin')
plt.title('Остатки линейной модели')
plt.show()

qq_data = residuals_lin

# Построение графика QQ
qqplot(qq_data, line='s')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Sample Quantiles')
plt.title('График QQ линейной модели')
plt.show()
print()
"""5. **Выбор оптимального набора регрессоров и учет структурной неоднородности**
* Сформулируйте гипотезу о наличии неоднородности в данных, оцените соответствующие модели и проведите тест Чоу, проверив целесообразность учета этой неоднородности. 
* Далее по результатам теста включите соответствующие дамми и перекрестные произведения. (*Подсказка: По результатам анализа описательных статистик выявите переменную, по которой наблюдается наиболее сильная неоднородность*)
* Используя тест Рамсея, тесты на мультиколлинеарность и тесты на группы лишних переменных, попробуйте найти оптимальный набор регрессоров для вашей модели. 
* Дайте содержательную интерпретацию наиболее адекватной модели из моделей, оцененных в этом разделе.
"""

sorted_data = data.copy()
sorted_data = data.sort_values('first_last_floor')
sorted_data['id'] = list(range(len(sorted_data)))
first_id = sorted_data.loc[sorted_data['first_last_floor'] == 1, 'id'].iloc[0]
print("Номер индекса строки для теста Чоу:", first_id)
print()
sorted_data.head()
print()

# Тест Чоу. Библиотека https://github.com/David-Woroniuk/chowtest
# Нужно pip install chowtest
# Работает на Убунту, но на Windows - иногда нет, иногда да. К сожалению, поздно заметили
# Если не работает, закомментить Чоу - можно открыть в Google Colab.
# https://colab.research.google.com/drive/1Gy1qjVljEG54RpMIGq5UQiKORFup2xhI

# print("Для первой модели")
# from chow_test import chow_test
# result = chow_test(y_series=sorted_data['price_doc'],
#                    X_series=sorted_data[['full_sq', 'life_sq', 'floor', 'max_floor', 'build_year', 'kremlin_km',
#                                          'public_transport_station_km', 'metro_km_walk', 'park_km', 'green_zone_part',
#                                          'green_zone_km']],
#                    last_index=268,
#                    first_index=269,
#                    significance=.05)
# print()
# print("Для второй модели")
# result = chow_test(y_series=sorted_data['price_doc'],
#                    X_series=sorted_data[['full_sq', 'first_last_floor', 'kremlin_km', 'metro_close', 'park_km']],
#                    last_index=268,
#                    first_index=269,
#                    significance=.05)
# print()

from patsy import dmatrices
from statsmodels.stats.outliers_influence import variance_inflation_factor

y, X = dmatrices(
    'price_doc ~ full_sq + life_sq + floor + max_floor + first_last_floor + build_year + public_transport_station_km + kremlin_km + metro_close + metro_km_walk + park_km + green_zone_part + green_zone_km',
    data=data, return_type='dataframe')
vif = pd.DataFrame()

vif['VAR'] = X.columns
vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

vif
print(vif)
print()

import statsmodels.stats.diagnostic as smdiag
import numpy as np
import statsmodels.api as sm
from statsmodels.stats.diagnostic import linear_reset

X = data[['full_sq', 'life_sq', 'floor', 'max_floor', 'first_last_floor', 'build_year', 'kremlin_km',
          'public_transport_station_km', 'metro_km_walk', 'metro_close', 'park_km', 'green_zone_part', 'green_zone_km']]
y = data["price_doc"]

results1 = sm.OLS(y, sm.add_constant(X)).fit()
residuals1 = results1.resid

res_np = sm.OLS(results1.model.endog, results1.model.exog).fit()
smdiag.linear_reset(res_np)

reset_test = smdiag.linear_reset(res_np)
print("Ramsey Reset Test:")
print(reset_test.summary())
print()
import numpy as np

X = data[['full_sq', 'first_last_floor', 'kremlin_km', 'metro_close', 'park_km']]
y = data["price_doc"]

results2 = sm.OLS(y, sm.add_constant(X)).fit()
residuals2 = results2.resid

res_np2 = sm.OLS(results2.model.endog, results2.model.exog).fit()
smdiag.linear_reset(res_np2)

reset_test2 = smdiag.linear_reset(res_np2)
print("Ramsey Reset Test:")
print(reset_test2.summary())
print()

"""6. **Выявление и коррекция гетероскедастичности и автокореляции**"""
breusch_pagan_test = sm.stats.diagnostic.het_breuschpagan(results1.resid, results1.model.exog)

print('Breusch-Pagan test statistic:', breusch_pagan_test[0])
print('Breusch-Pagan p-value:', breusch_pagan_test[1])
print()
breusch_pagan_test = sm.stats.diagnostic.het_breuschpagan(results2.resid, results2.model.exog)

print('Breusch-Pagan test statistic:', breusch_pagan_test[0])
print('Breusch-Pagan p-value:', breusch_pagan_test[1])
print()
import statsmodels.api as sm

durbin_watson_statistic = sm.stats.stattools.durbin_watson(residuals1)
print("Durbin-Watson statistic:", durbin_watson_statistic)

import statsmodels.api as sm

durbin_watson_statistic = sm.stats.stattools.durbin_watson(residuals2)
print("Durbin-Watson statistic:", durbin_watson_statistic)

"""7. **Построение прогноза**

* На основании полученных моделей постройте прогноз на тестовых данных.
* Рассчитайте MAPE и RMSE. 
* Сделайте выводы.
"""
y_test = np.exp(y_test)

from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error

X_test = X_test[['full_sq', 'life_sq', 'floor', 'max_floor', 'first_last_floor', 'build_year', 'kremlin_km',
                 'public_transport_station_km', 'metro_km_walk', 'metro_close', 'park_km', 'green_zone_part',
                 'green_zone_km']]
X_test = sm.add_constant(X_test)
y_pred = results1.predict(X_test)

mape = mean_absolute_percentage_error(y_test, np.exp(y_pred))
rmse = np.sqrt(mean_squared_error(y_test, np.exp(y_pred)))
print()
print("Прогноз первой модели:")
print("MAPE:", mape)
print("RMSE:", rmse)
print()
frame = {'Actual': y_test.sort_index(), 'Predicted': round(np.exp(y_pred.sort_index()))}
results = pd.DataFrame(frame)
print(results)
print()
# Просмотр полной таблицы факта и прогноза для первой модели
# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also
#     print(results)

from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error

X_test = X_test[['full_sq', 'first_last_floor', 'kremlin_km', 'metro_close', 'park_km']]
X_test = sm.add_constant(X_test)

y_pred = results2.predict(X_test)
mape = mean_absolute_percentage_error(y_test, np.exp(y_pred))
y_pred = np.exp(y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Прогноз второй модели:")
print("MAPE:", mape)
print("RMSE:", rmse)
print()
frame = {'Actual': y_test.sort_index(), 'Predicted': round(y_pred.sort_index())}
results = pd.DataFrame(frame)
print(results)
print()
# Просмотр полной таблицы факта и прогноза для второй модели
# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also
#     print(results)

"""8. **Бонусный пункт:** сравнение с моделями машинного обучения

* Выберите модели машинного обучения на свое усмотрение. Обучите модели на трех типах данных, на сырых, очищенных от выбросов, полученных на основании обучающей выборки (они должны совпадать с данными использованными в предыдущих пунктах). 
* На основании моделей постройте прогноз на тестовых данных. Рассчитайте MAPE и RMSE.
* Сравните с эконометрическими моделями. Сделайте выводы
"""
print("Случайный лес, сырые данные")
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error

model = RandomForestRegressor()

model.fit(X_train_raw, y_train_raw)

y_pred_raw = model.predict(X_test_raw)

mape_raw = mean_absolute_percentage_error(y_test_raw, y_pred_raw)
rmse_raw = mean_squared_error(y_test_raw, y_pred_raw, squared=False)

print("MAPE (сырые данные):", mape_raw)
print("RMSE (сырые данные):", rmse_raw)

frame = {'Actual': y_test_raw.sort_index(), 'Predicted': y_pred_raw}
results = pd.DataFrame(frame)
print(results)
print()
print("Случайный лес, очищенные данные")
y_train_clean = np.exp(y_train_clean)
y_test_clean = np.exp(y_test_clean)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error

model = RandomForestRegressor()

model.fit(X_train_clean, y_train_clean)
y_pred_clean = model.predict(X_test_clean)

mape_clean = mean_absolute_percentage_error(y_test_clean, y_pred_clean)
rmse_clean = mean_squared_error(y_test_clean, y_pred_clean, squared=False)

print("MAPE (очищенные данные):", mape_clean)
print("RMSE (очищенные данные):", rmse_clean)

frame = {'Actual': y_test_clean.sort_index(), 'Predicted': y_pred_clean}
results = pd.DataFrame(frame)
print(results)
print()
model = RandomForestRegressor()
print("Случайный лес, обучение на полученной модели лин. регрессии")
model.fit(X_test_clean, y_pred)
y_pred_final = model.predict(X_test_clean)

mape_final = mean_absolute_percentage_error(y_pred, y_pred_final)
rmse_final = mean_squared_error(y_pred, y_pred_final, squared=False)

print("MAPE (результаты обучающей выборки):", mape_final)
print("RMSE (результаты обучающей выборки):", rmse_final)

frame = {'Actual': y_pred.sort_index(), 'Predicted': y_pred_final}
results = pd.DataFrame(frame)
print(results)
print()

print("Градиентный бустинг, сырые данные")
from sklearn.ensemble import GradientBoostingRegressor

model = GradientBoostingRegressor()

model.fit(X_train_raw, y_train_raw)

y_pred_raw = model.predict(X_test_raw)

mape_raw = mean_absolute_percentage_error(y_test_raw, y_pred_raw)
rmse_raw = mean_squared_error(y_test_raw, y_pred_raw, squared=False)

print("MAPE (сырые данные):", mape_raw)
print("RMSE (сырые данные):", rmse_raw)

frame = {'Actual': y_test_raw.sort_index(), 'Predicted': y_pred_raw}
results = pd.DataFrame(frame)
print(results)
print()
print("Градиентный бустинг, очищенные данные")
model = GradientBoostingRegressor()

model.fit(X_train_clean, y_train_clean)
y_pred_clean = model.predict(X_test_clean)

mape_clean = mean_absolute_percentage_error(y_test_clean, y_pred_clean)
rmse_clean = mean_squared_error(y_test_clean, y_pred_clean, squared=False)

print("MAPE (очищенные данные):", mape_clean)
print("RMSE (очищенные данные):", rmse_clean)

frame = {'Actual': y_test_clean.sort_index(), 'Predicted': y_pred_clean}
results = pd.DataFrame(frame)
print(results)
print()
print("Градиентный бустинг, обучение на полученной модели лин. регрессии")
model = GradientBoostingRegressor()

model.fit(X_test_clean, y_pred)
y_pred_final = model.predict(X_test_clean)

mape_final = mean_absolute_percentage_error(y_pred, y_pred_final)
rmse_final = mean_squared_error(y_pred, y_pred_final, squared=False)

print("MAPE (результаты обучающей выборки):", mape_final)
print("RMSE (результаты обучающей выборки):", rmse_final)

frame = {'Actual': y_pred.sort_index(), 'Predicted': y_pred_final}
results = pd.DataFrame(frame)
print(results)
print()

"""9. **Заключение**

* Коротко опишите, какие проблемы с данными были выявлены в ходе анализа. Приведите сводную таблицу результатов оцененных моделей. 
* * Для каких показателей наблюдается стабильность оценок? 
* * Какая модель представляется наиболее убедительной? Дайте ее содержательную интерпретацию.
"""

# Оценки отклонен
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X_train, np.exp(y_train), cv=5)
mean_score = np.mean(scores)
std_score = np.std(scores)
print("Среднее значение оценок:", mean_score)
print("Стандартное отклонение оценок:", std_score)

print()
model_results = [
    {'Модель': 'Линейная регрессия, модель 2', 'MAPE': 38.02331362539498, 'RMSE': 1675587.4743598334},
    {'Модель': 'Линейная регрессия, модель 1', 'MAPE': 491.2799656806219, 'RMSE': 101202707.28882022},
    {'Модель': 'Random Forest', 'MAPE': 2.1339863338400756, 'RMSE': 210972.74206231974},
    {'Модель': 'Градиентный бустинг', 'MAPE': 0.333177307247896, 'RMSE': 27171.6244088889}
]

format_decimal = lambda x: '{:.2f}'.format(x)
format_integer = lambda x: '{:.0f}'.format(x)

results_df = pd.DataFrame(model_results)

summary_table = results_df.pivot_table(index='Модель', values=['MAPE', 'RMSE'])
summary_table['MAPE'] = summary_table['MAPE'].apply(format_decimal)
summary_table['RMSE'] = summary_table['RMSE'].apply(format_integer)

print(summary_table)